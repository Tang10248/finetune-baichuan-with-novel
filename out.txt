nohup: 忽略输入
W0101 23:10:49.569774 126762856138560 torch/distributed/run.py:757] 
W0101 23:10:49.569774 126762856138560 torch/distributed/run.py:757] *****************************************
W0101 23:10:49.569774 126762856138560 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0101 23:10:49.569774 126762856138560 torch/distributed/run.py:757] *****************************************
world size 8 local rank 0
local rank 0 map {'': 0}
load model with 8bit quantization
world size 8 local rank 1
local rank 1 map {'': 1}
load model with 8bit quantization
world size 8 local rank 6
local rank 6 map {'': 6}
load model with 8bit quantization
world size 8 local rank 5
local rank 5 map {'': 5}
load model with 8bit quantization
world size 8 local rank 3
local rank 3 map {'': 3}
load model with 8bit quantization
world size 8 local rank 2
local rank 2 map {'': 2}
load model with 8bit quantization
world size 8 local rank 7
local rank 7 map {'': 7}
load model with 8bit quantization
world size 8 local rank 4
local rank 4 map {'': 4}
load model with 8bit quantization
pass unk_token_id 0 to pad_token_id
memory usage of model: 7.1 GB
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
['up_proj', 'gate_proj', 'W_pack', 'o_proj', 'down_proj']
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'up_proj', 'gate_proj', 'W_pack', 'o_proj', 'down_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))
[rank1]: Traceback (most recent call last):
[rank1]:   File "train.py", line 119, in <module>
[rank1]:     main()
[rank1]:   File "train.py", line 87, in main
[rank1]:     model = get_peft_model(model, config)
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/mapping.py", line 193, in get_peft_model
[rank1]:     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 1609, in __init__
[rank1]:     super().__init__(model, peft_config, adapter_name, **kwargs)
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 171, in __init__
[rank1]:     self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 141, in __init__
[rank1]:     super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 184, in __init__
[rank1]:     self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 496, in inject_adapter
[rank1]:     self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 226, in _create_and_replace
[rank1]:     new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 344, in _create_new_module
[rank1]:     new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)
[rank1]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/bnb.py", line 273, in dispatch_bnb_8bit
[rank1]:     "memory_efficient_backward": target.state.memory_efficient_backward,
[rank1]: AttributeError: 'MatmulLtState' object has no attribute 'memory_efficient_backward'
pass unk_token_id 0 to pad_token_id
memory usage of model: 7.1 GB
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
['o_proj', 'up_proj', 'gate_proj', 'W_pack', 'down_proj']
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'o_proj', 'up_proj', 'gate_proj', 'W_pack', 'down_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))
[rank5]: Traceback (most recent call last):
[rank5]:   File "train.py", line 119, in <module>
[rank5]:     main()
[rank5]:   File "train.py", line 87, in main
[rank5]:     model = get_peft_model(model, config)
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/mapping.py", line 193, in get_peft_model
[rank5]:     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 1609, in __init__
[rank5]:     super().__init__(model, peft_config, adapter_name, **kwargs)
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 171, in __init__
[rank5]:     self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 141, in __init__
[rank5]:     super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 184, in __init__
[rank5]:     self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 496, in inject_adapter
[rank5]:     self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 226, in _create_and_replace
[rank5]:     new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 344, in _create_new_module
[rank5]:     new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)
[rank5]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/bnb.py", line 273, in dispatch_bnb_8bit
[rank5]:     "memory_efficient_backward": target.state.memory_efficient_backward,
[rank5]: AttributeError: 'MatmulLtState' object has no attribute 'memory_efficient_backward'
pass unk_token_id 0 to pad_token_id
memory usage of model: 7.1 GB
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
['o_proj', 'W_pack', 'up_proj', 'down_proj', 'gate_proj']
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'o_proj', 'W_pack', 'up_proj', 'down_proj', 'gate_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))
[rank0]: Traceback (most recent call last):
[rank0]:   File "train.py", line 119, in <module>
[rank0]:     main()
[rank0]:   File "train.py", line 87, in main
[rank0]:     model = get_peft_model(model, config)
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/mapping.py", line 193, in get_peft_model
[rank0]:     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 1609, in __init__
[rank0]:     super().__init__(model, peft_config, adapter_name, **kwargs)
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 171, in __init__
[rank0]:     self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 141, in __init__
[rank0]:     super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 184, in __init__
[rank0]:     self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 496, in inject_adapter
[rank0]:     self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 226, in _create_and_replace
[rank0]:     new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 344, in _create_new_module
[rank0]:     new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)
[rank0]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/bnb.py", line 273, in dispatch_bnb_8bit
[rank0]:     "memory_efficient_backward": target.state.memory_efficient_backward,
[rank0]: AttributeError: 'MatmulLtState' object has no attribute 'memory_efficient_backward'
pass unk_token_id 0 to pad_token_id
memory usage of model: 7.1 GB
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
['gate_proj', 'o_proj', 'down_proj', 'up_proj', 'W_pack']
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'gate_proj', 'o_proj', 'down_proj', 'up_proj', 'W_pack'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))
[rank7]: Traceback (most recent call last):
[rank7]:   File "train.py", line 119, in <module>
[rank7]:     main()
[rank7]:   File "train.py", line 87, in main
[rank7]:     model = get_peft_model(model, config)
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/mapping.py", line 193, in get_peft_model
[rank7]:     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 1609, in __init__
[rank7]:     super().__init__(model, peft_config, adapter_name, **kwargs)
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 171, in __init__
[rank7]:     self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 141, in __init__
[rank7]:     super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 184, in __init__
[rank7]:     self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 496, in inject_adapter
[rank7]:     self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 226, in _create_and_replace
[rank7]:     new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 344, in _create_new_module
[rank7]:     new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)
[rank7]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/bnb.py", line 273, in dispatch_bnb_8bit
[rank7]:     "memory_efficient_backward": target.state.memory_efficient_backward,
[rank7]: AttributeError: 'MatmulLtState' object has no attribute 'memory_efficient_backward'
pass unk_token_id 0 to pad_token_id
memory usage of model: 7.1 GB
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
['o_proj', 'gate_proj', 'up_proj', 'W_pack', 'down_proj']
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'o_proj', 'gate_proj', 'up_proj', 'W_pack', 'down_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))
[rank6]: Traceback (most recent call last):
[rank6]:   File "train.py", line 119, in <module>
[rank6]:     main()
[rank6]:   File "train.py", line 87, in main
[rank6]:     model = get_peft_model(model, config)
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/mapping.py", line 193, in get_peft_model
[rank6]:     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 1609, in __init__
[rank6]:     super().__init__(model, peft_config, adapter_name, **kwargs)
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/peft_model.py", line 171, in __init__
[rank6]:     self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 141, in __init__
[rank6]:     super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 184, in __init__
[rank6]:     self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 496, in inject_adapter
[rank6]:     self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 226, in _create_and_replace
[rank6]:     new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 344, in _create_new_module
[rank6]:     new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)
[rank6]:   File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/peft/tuners/lora/bnb.py", line 273, in dispatch_bnb_8bit
[rank6]:     "memory_efficient_backward": target.state.memory_efficient_backward,
[rank6]: AttributeError: 'MatmulLtState' object has no attribute 'memory_efficient_backward'
W0101 23:11:24.702898 126762856138560 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1483407 closing signal SIGTERM
W0101 23:11:24.704143 126762856138560 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1483409 closing signal SIGTERM
W0101 23:11:24.704499 126762856138560 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1483410 closing signal SIGTERM
W0101 23:11:24.704840 126762856138560 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1483411 closing signal SIGTERM
W0101 23:11:24.705153 126762856138560 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1483413 closing signal SIGTERM
W0101 23:11:24.705452 126762856138560 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1483414 closing signal SIGTERM
E0101 23:11:25.586830 126762856138560 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 1 (pid: 1483408) of binary: /home/4T/tangshunye/.conda/envs/ft-bc/bin/python
Traceback (most recent call last):
  File "/home/4T/tangshunye/.conda/envs/ft-bc/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/4T/tangshunye/.conda/envs/ft-bc/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-01-01_23:11:24
  host      : zhn-NF5468M5-S
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 1483412)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-01_23:11:24
  host      : zhn-NF5468M5-S
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1483408)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
